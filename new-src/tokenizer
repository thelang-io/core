/*!
 * Copyright (c) 2018 Aaron Delasy
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

obj Tokenizer {
  reader: ref Reader
  state: TokenizerState
  data: Token[]
  errors: str[]
}

obj TokenizerState {
  idx: int
  pos: int
  ch: char
  handled: bool
}

fn Tokenizer_init (mut reader: ref Reader) Tokenizer {
  return Tokenizer{
    reader: reader,
    state: TokenizerState{
      idx: 0,
      pos: 0,
      ch: '\0',
      handled: false
    },
    data: [],
    errors: []
  }
}

fn Tokenizer_next (
  mut self: ref Tokenizer,
  withIgnored := false
) Token {
  if self.data.len < self.state.idx {
    return self.data[self.state.idx++]
  } elif self.data[self.data.len - 1].type == TK_EOF {
    RaiseError("Tried to tokenize on eof")
  }

  loop {
    tok := _Tokenizer_getToken(self)

    self.data.push(tok)
    self.state.idx++

    if withIgnored || (
      tok.type != TK_WHITESPACE &&
      tok.type != TK_COMMENT_BLOCK &&
      tok.type != TK_COMMENT_LINE
    ) {
      return tok
    }
  }
}

fn _Tokenizer_getToken (mut self: ref Tokenizer) Token {
  if Reader_eof(self.reader) {
    return Token{
      type: TK_EOF,
      val: "",
      start: self.state.pos,
      end: self.state.pos
    }
  }

  self.state.ch = Reader_next(self.reader)
  self.state.handled = false
  mut tok: Token?

  if (tok = _Tokenizer_maybeWhitespace(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeCommentBlock(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeCommentLine(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeOp(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeKeyword(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeNumber(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeString(self)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeChar(self)) != nil {
    return tok
  } else {
    _Tokenizer_raise(self, E0000(ch), self.state.pos)
    return _Tokenizer_wrapToken(self, TK_UNKNOWN)
  }
}

fn _Tokenizer_maybeChar (mut self: ref Tokenizer) Token? {
  if self.state.ch != '\'' {
    return nil
  }

  if Reader_eof(self.reader) {
    _Tokenizer_raise(self, E0002(), self.state.pos)
    return _Tokenizer_wrapToken(self, TK_LIT_CHAR)
  }

  ch1 := Reader_next(self.reader)

  if ch1 == '\n' {
    _Tokenizer_raise(self, E0002(), self.state.pos)
    return _Tokenizer_wrapToken(self, TK_LIT_CHAR)
  } elif ch1 == '\'' {
    _Tokenizer_raise(self, E0004(), self.state.pos)
    return _Tokenizer_wrapToken(self, TK_LIT_CHAR)
  } elif ch1 == '\\' {
    if Reader_eof(self.reader) {
      _Tokenizer_raise(self, E0002(), self.state.pos)
      return _Tokenizer_wrapToken(self, TK_LIT_CHAR)
    }

    ch2 := Reader_next(self.reader)

    if !Token_isCharEsc(ch2) {
      _Tokenizer_raise(self, E0005(), self.state.pos)
    }
  }

  if !Reader_lookahead(self.reader, '\'') {
    loop {
      if Reader_eof(self.reader) {
        _Tokenizer_raise(self, E0002(), self.state.pos)
        break
      }

      pos3 := self.reader.pos
      ch3 := Reader_next(self.reader)

      if ch3 == '\'' {
        _Tokenizer_raise(self, E0007(), self.state.pos)
        break
      } elif ch3 == '\n' {
        Reader_seek(self.reader, pos3)
        _Tokenizer_raise(self, E0002(), self.state.pos)

        break
      }
    }
  }

  return _Tokenizer_wrapToken(self, TK_LIT_CHAR)
}

fn _Tokenizer_maybeCommentBlock (mut self: ref Tokenizer) Token? {
  if self.state.ch != '/' || !Reader_lookahead(self.reader, '*') {
    return nil
  }

  loop {
    if Reader_eof(self.reader) {
      _Tokenizer_raise(self, E0001(), self.state.pos)
      break
    }

    ch := Reader_next(self.reader)

    if ch == '*' && Reader_lookahead(self.reader, '/') {
      break
    }
  }

  return _Tokenizer_wrapToken(self, TK_COMMENT_BLOCK)
}

fn _Tokenizer_maybeCommentLine (mut self: ref Tokenizer) Token? {
  if self.state.ch != '/' || !Reader_lookahead(self.reader, '/') {
    return nil
  }

  Reader_walk(self.reader, Token_isNotNewLine)
  return _Tokenizer_wrapToken(self, TK_COMMENT_LINE)
}

fn _Tokenizer_maybeKeyword (mut self: ref Tokenizer) Token? {
  if !Token_isIdStart(self.state.ch) {
    return nil
  }

  Reader_walk(self.reader, Token_isId)
  val := Reader_slice(self.reader, self.state.pos, self.reader.pos)

  if val == "break" {
    return _Tokenizer_wrapToken(self, TK_KW_BREAK)
  } elif val == "catch" {
    return _Tokenizer_wrapToken(self, TK_KW_CATCH)
  } elif val == "continue" {
    return _Tokenizer_wrapToken(self, TK_KW_CONTINUE)
  } elif val == "elif" {
    return _Tokenizer_wrapToken(self, TK_KW_ELIF)
  } elif val == "else" {
    return _Tokenizer_wrapToken(self, TK_KW_ELSE)
  } elif val == "enum" {
    return _Tokenizer_wrapToken(self, TK_KW_ENUM)
  } elif val == "export" {
    return _Tokenizer_wrapToken(self, TK_KW_EXPORT)
  } elif val == "false" {
    return _Tokenizer_wrapToken(self, TK_KW_FALSE)
  } elif val == "fn" {
    return _Tokenizer_wrapToken(self, TK_KW_FN)
  } elif val == "if" {
    return _Tokenizer_wrapToken(self, TK_KW_IF)
  } elif val == "import" {
    return _Tokenizer_wrapToken(self, TK_KW_IMPORT)
  } elif val == "is" {
    return _Tokenizer_wrapToken(self, TK_KW_IS)
  } elif val == "loop" {
    return _Tokenizer_wrapToken(self, TK_KW_LOOP)
  } elif val == "main" {
    return _Tokenizer_wrapToken(self, TK_KW_MAIN)
  } elif val == "mut" {
    return _Tokenizer_wrapToken(self, TK_KW_MUT)
  } elif val == "nil" {
    return _Tokenizer_wrapToken(self, TK_KW_NIL)
  } elif val == "obj" {
    return _Tokenizer_wrapToken(self, TK_KW_OBJ)
  } elif val == "ref" {
    return _Tokenizer_wrapToken(self, TK_KW_REF)
  } elif val == "return" {
    return _Tokenizer_wrapToken(self, TK_KW_RETURN)
  } elif val == "throw" {
    return _Tokenizer_wrapToken(self, TK_KW_THROW)
  } elif val == "true" {
    return _Tokenizer_wrapToken(self, TK_KW_TRUE)
  } elif val == "try" {
    return _Tokenizer_wrapToken(self, TK_KW_TRY)
  } elif val == "union" {
    return _Tokenizer_wrapToken(self, TK_KW_UNION)
  }

  return _Tokenizer_wrapToken(self, TK_ID)
}

fn _Tokenizer_maybeNumber (mut self: ref Tokenizer) Token? {
  if !self.state.ch.isDigit() {
    return nil
  }

  if self.state.ch == '0' {
    if Reader_eof(self.reader) {
      return _Tokenizer_wrapToken(self, TK_LIT_INT_DEC)
    }

    pos := self.reader.pos
    ch := Reader_next(self.reader)

    if Token_isIntDec(ch) {
      Reader_walk(self.reader, Token_isId)
      _Tokenizer_raise(self, E0008(), self.state.pos)

      return _Tokenizer_wrapToken(self, TK_LIT_INT_DEC)
    } elif ch == 'B' || ch == 'b' {
      return _Tokenizer_wrapInt(
        self,
        TK_LIT_INT_BIN,
        E0009("binary"),
        Token_isIntBin
      )
    } elif ch == 'O' || ch == 'o' {
      return _Tokenizer_wrapInt(
        self,
        TK_LIT_INT_OCT,
        E009("octal"),
        Token_isIntOct
      )
    } elif ch == 'X' || ch == 'x' {
      return _Tokenizer_wrapInt(
        self,
        TK_LIT_INT_HEX,
        E0009("hexadecimal"),
        Token_isIntHex
      )
    }

    Reader_seek(self.reader, pos)
  } else {
    Reader_walk(self.reader, Token_isIntDec)
  }

  return _Tokenizer_wrapInt(
    self,
    TK_LIT_INT_DEC,
    E009("decimal"),
    Token_isIntDec
  )
}

fn _Tokenizer_maybeOp (mut self: ref Tokenizer) Token? {
  if self.state.ch == '&' {
    if Reader_lookahead(self.reader, '&') {
      if Reader_lookahead(self.reader, '=') {
        return _Tokenizer_wrapToken(self, TK_OP_AMP_AMP_EQ)
      } else {
        return _Tokenizer_wrapToken(self, TK_OP_AMP_AMP)
      }
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_AMP_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_AMP)
    }
  } elif self.state.ch == '@' {
    return _Tokenizer_wrapToken(self, TK_OP_AT)
  } elif self.state.ch == '`' {
    return _Tokenizer_wrapToken(self, TK_OP_BACKTICK)
  } elif self.state.ch == '\\' {
    return _Tokenizer_wrapToken(self, TK_OP_BACKSLASH)
  } elif self.state.ch == '^' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_CARET_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_CARET)
    }
  } elif self.state.ch == ':' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_COLON_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_COLON)
    }
  } elif self.state.ch == ',' {
    return _Tokenizer_wrapToken(self, TK_OP_COMMA)
  } elif self.state.ch == '.' {
    pos := self.reader.pos

    if (
      Reader_lookahead(self.reader, '.') &&
      Reader_lookahead(self.reader, '.')
    ) {
      return _Tokenizer_wrapToken(self, TK_OP_ELLIPSIS)
    }

    Reader_seek(self.reader, pos)
    return _Tokenizer_wrapToken(self, TK_OP_DOT)
  } elif self.state.ch == '$' {
    return _Tokenizer_wrapToken(self, TK_OP_DOLLAR)
  } elif self.state.ch == '=' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_EQ_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_EQ)
    }
  } elif self.state.ch == '!' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_EXCL_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_EXCL)
    }
  } elif self.state.ch == '>' {
    if Reader_lookahead(self.reader, '>') {
      if Reader_lookahead(self.reader, '=') {
        return _Tokenizer_wrapToken(self, TK_OP_RSHIFT_EQ)
      } else {
        return _Tokenizer_wrapToken(self, TK_OP_RSHIFT)
      }
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_GT_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_GT)
    }
  } elif self.state.ch == '#' {
    return _Tokenizer_wrapToken(self, TK_OP_HASH)
  } elif self.state.ch == '{' {
    return _Tokenizer_wrapToken(self, TK_OP_LBRACE)
  } elif self.state.ch == '[' {
    return _Tokenizer_wrapToken(self, TK_OP_LBRACK)
  } elif self.state.ch == '(' {
    return _Tokenizer_wrapToken(self, TK_OP_LPAR)
  } elif self.state.ch == '<' {
    if Reader_lookahead(self.reader, '<') {
      if Reader_lookahead(self.reader, '=') {
        return _Tokenizer_wrapToken(self, TK_OP_LSHIFT_EQ)
      } else {
        return _Tokenizer_wrapToken(self, TK_OP_LSHIFT)
      }
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_LT_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_LT)
    }
  } elif self.state.ch == '-' {
    if Reader_lookahead(self.reader, '-') {
      return _Tokenizer_wrapToken(self, TK_OP_MINUS_MINUS)
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_MINUS_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_MINUS)
    }
  } elif self.state.ch == '|' {
    if Reader_lookahead(self.reader, '|') {
      if Reader_lookahead(self.reader, '=') {
        return _Tokenizer_wrapToken(self, TK_OP_PIPE_PIPE_EQ)
      } else {
        return _Tokenizer_wrapToken(self, TK_OP_PIPE_PIPE)
      }
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_PIPE_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_PIPE)
    }
  } elif self.state.ch == '%' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_PERCENT_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_PERCENT)
    }
  } elif self.state.ch == '+' {
    if Reader_lookahead(self.reader, '+') {
      return _Tokenizer_wrapToken(self, TK_OP_PLUS_PLUS)
    } elif Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_PLUS_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_PLUS)
    }
  } elif self.state.ch == '?' {
    return _Tokenizer_wrapToken(self, TK_OP_QN)
  } elif self.state.ch == '}' {
    return _Tokenizer_wrapToken(self, TK_OP_RBRACE)
  } elif self.state.ch == ']' {
    return _Tokenizer_wrapToken(self, TK_OP_RBRACK)
  } elif self.state.ch == ')' {
    return _Tokenizer_wrapToken(self, TK_OP_RPAR)
  } elif self.state.ch == ';' {
    return _Tokenizer_wrapToken(self, TK_OP_SEMI)
  } elif self.state.ch == '/' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_SLASH_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_SLASH)
    }
  } elif self.state.ch == '*' {
    if Reader_lookahead(self.reader, '=') {
      return _Tokenizer_wrapToken(self, TK_OP_STAR_EQ)
    } else {
      return _Tokenizer_wrapToken(self, TK_OP_STAR)
    }
  } elif self.state.ch == '~' {
    return _Tokenizer_wrapToken(self, TK_OP_TILDE)
  }

  return nil
}

fn _Tokenizer_maybeString (mut self: ref Tokenizer) Token? {
  if self.state.ch != '"' {
    return nil
  }

  loop {
    if Reader_eof(self.reader) {
      _Tokenizer_raise(self, E0003(), self.state.pos)
      break
    }

    pos1 := self.reader.pos
    ch1 := Reader_next(self.reader)

    if ch1 == '"' {
      break
    } elif ch1 == '\\' {
      ch2 := Reader_next(self.reader)

      if !Token_isStrEsc(ch2) {
        _Tokenizer_raise(self, E0006(), pos1)
      }
    }
  }

  return _Tokenizer_wrapToken(self, TK_LIT_STR)
}

fn _Tokenizer_maybeWhitespace (mut self: ref Tokenizer) Token? {
  if !self.state.ch.isSpace() {
    return nil
  }

  Reader_walk(self.reader, Token_isWhitespace)
  return _Tokenizer_wrapToken(self, TK_WHITESPACE)
}

fn _Tokenizer_raise (mut self: ref Tokenizer, message: str, start: int) {
  if self.state.handled {
    return
  }

  startLoc := Location_locate(self.reader.content, start)
  endLoc := Location_locate(self.reader.content, self.reader.pos)
  lines := self.reader.content.lines()

  mut errorLines: str[]
  mut errorLinesNums: int[]

  if startLoc.line != 1 {
    errorLines.push(lines[startLoc.line - 2])
    errorLinesNums.push(startLoc.line - 2)
  }

  errorLines.push(lines[startLoc.line - 1])
  errorLinesNums.push(startLoc.line - 1)

  if startLoc.line == endLoc.line && startLoc.col == endLoc.col {
    errorLines.push(' '.repeat(startLoc.col - 1) + '^')
  } elif startLoc.line == endLoc.line {
    errorLines.push(
      ' '.repeat(startLoc.col - 1) +
      '~'.repeat(endLoc.col - startLoc.col)
    )
  } else {
    errorLine := lines[startLoc.line - 1]

    errorLines.push(
      ' '.repeat(startLoc.col - 1) +
      '~'.repeat(errorLine.len - startLoc.col)
    )
  }

  errorLinesNums.push(startLoc.line - 1)

  if startLoc.line != lines.len {
    errorLines.push(lines[startLoc.line])
    errorLinesNums.push(startLoc.line)
  }

  gutterLen := errorLinesNums[errorLinesNums.len - 1].str().len + 1

  mut error := self.reader.path + ":" + startLoc.line.str() + ":" +
    startLoc.col.str() + ": SyntaxError: " + message + os_EOL

  mut prevLineNum := 0

  loop i := 0; i < errorLinesNums.len; i++ {
    errorLineNum := errorLinesNums[i]
    sameLine := prevLineNum == errorLineNum

    if sameLine {
      error += ' '.repeat(gutterLen)
    } else {
      errorLineNumLen := errorLineNum.str().len
      error += errorLineNum.str() + ' '.repeat(gutterLen - errorLineNumLen)
    }

    error += "| " + errorLines[i]
    prevLineNum = errorLineNum
  }

  self.errors.push(error)
  self.state.handled = true
}

fn _Tokenizer_wrapFloat (mut self: ref Tokenizer, type: int, errorMessage: str) Token {
  if Reader_eof(self.reader) {
    return _Tokenizer_wrapToken(self, type)
  }

  pos1 := self.reader.pos
  ch1 := Reader_next(self.reader)

  if Token_isId(ch1) && ch1 != 'E' && ch1 != 'e' {
    Reader_walk(self.reader, Token_isId)
    _Tokenizer_raise(self, errorMessage, self.state.pos)

    return _Tokenizer_wrapToken(self, type)
  } elif ch1 != '.' && ch1 != 'E' && ch1 != 'e' {
    Reader_seek(self.reader, pos1)
    return _Tokenizer_wrapToken(self, type)
  }

  mut expStartPos := pos1

  if ch1 == '.' {
    if Reader_eof(self.reader) {
      _Tokenizer_raise(self, E0010(), self.state.pos)
      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    }

    pos2 := self.reader.pos
    ch2 := Reader_next(self.reader)

    if ch2 == '.' {
      Reader_seek(self.reader, pos1)
      return _Tokenizer_wrapToken(self, type)
    } elif !Token_isIntDec(ch2) {
      Reader_walk(self.reader, Token_isId)
      _Tokenizer_raise(self, E0010(), self.state.pos)

      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    }

    Reader_walk(self.reader, Token_isIntDec)

    if Reader_eof(self.reader) {
      return _Tokenizer_wrapTokenFloat(self, type)
    }

    pos3 := self.reader.pos
    ch3 := Reader_next(self.reader)

    if Token_isId(ch3) && ch3 != 'E' && ch3 != 'e' {
      Reader_walk(self.reader, Token_isId)
      _Tokenizer_raise(self, E0010(), self.state.pos)

      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    } elif ch3 != 'E' && ch3 != 'e' {
      Reader_seek(self.reader, pos2)
      return _Tokenizer_wrapTokenFloat(self, type)
    }

    expStartPos = pos3
  }

  if Reader_eof(self.reader) {
    _Tokenizer_raise(self, E0011(), expStartPos)
    return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
  }

  ch4 := Reader_next(self.reader)

  if !Token_isIntDec(ch4) && ch4 != '+' && ch4 != '-' {
    Reader_walk(self.reader, Token_isId)
    _Tokenizer_raise(self, E0011(), expStartPos)

    return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
  }

  if ch4 == '+' || ch4 == '-' {
    if Reader_eof(self.reader) {
      _Tokenizer_raise(self, E0011(), expStartPos)
      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    }

    ch5 := Reader_next(self.reader)

    if !Token_isIntDec(ch5) {
      Reader_walk(self.reader, Token_isId)
      _Tokenizer_raise(self, E0011(), expStartPos)

      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    }
  }

  Reader_walk(self.reader, Token_isIntDec)
  return _Tokenizer_wrapTokenFloat(self, type)
}

fn _Tokenizer_wrapInt (
  mut self: ref Tokenizer,
  type: int,
  errorMessage: str,
  check: fn (char) bool
) Token {
  if Reader_eof(self.reader) {
    _Tokenizer_raise(self, errorMessage, self.state.pos)
    return _Tokenizer_wrapToken(self, type)
  }

  ch := Reader_next(self.reader)

  if !check(ch) {
    Reader_walk(self.reader, Token_isId)
    _Tokenizer_raise(self, errorMessage, self.state.pos)

    return _Tokenizer_wrapToken(self, type)
  }

  Reader_walk(self.reader, check)
  return _Tokenizer_wrapFloat(self, type, errorMessage)
}

fn _Tokenizer_wrapToken (mut self: ref Tokenizer, type: int) Token {
  start := self.state.pos
  self.state.pos = self.reader.pos
  self.state.handled = true

  return Token{
    type: type,
    val: Reader_slice(self.reader, start, self.state.pos),
    start: start,
    end: self.state.pos
  }
}

fn _Tokenizer_wrapTokenFloat (mut self: ref Tokenizer, type: int) Token {
  if !Reader_eof(self.reader) {
    ch := Reader_next(self.reader)

    if Token_isId(ch) {
      Reader_walk(self.reader, Token_isId)
      _Tokenizer_raise(self, E0010(), self.state.pos)

      return _Tokenizer_wrapToken(self, TK_LIT_FLOAT)
    }

    Reader_seek(self.reader, pos)
  }

  if type == TK_LIT_INT_BIN {
    _Tokenizer_raise(self, E0014("binary"), self.state.pos)
  } elif type == TK_LIT_INT_HEX {
    _Tokenizer_raise(self, E0014("hexadecimal"), self.state.pos)
  } elif type == TK_LIT_INT_OCT {
    _Tokenizer_raise(self, E0014("octal"), self.state.pos)
  }

  return _Tokenizer_wrapToken(TK_LIT_FLOAT)
}
