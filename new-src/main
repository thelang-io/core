/*!
 * Copyright (c) Aaron Delasy
 *
 * Unauthorized copying of this file, via any medium is strictly prohibited
 * Proprietary and confidential
 */

fn RaiseError (message: str) void {
  print(message, to: "stderr")
  exit(1)
}

obj Arguments {
  fileName: str
}

obj Location {
  line: int
  col: int
}

obj Reader {
  path: str
  content: str
  pos: int
}

obj Token {
  type: TokenType
  val: str
  start: int
  end: int
}

enum TokenAssociativity {
  TK_ASSOC_NONE,
  TK_ASSOC_LEFT,
  TK_ASSOC_RIGHT
}

enum TokenType {
  TK_UNKNOWN,
  TK_EOF,
  TK_WHITESPACE,

  TK_COMMENT_BLOCK,
  TK_COMMENT_LINE,

  TK_ID,

  TK_KW_BREAK,
  TK_KW_CATCH,
  TK_KW_CONTINUE,
  TK_KW_ELIF,
  TK_KW_ELSE,
  TK_KW_ENUM,
  TK_KW_EXPORT,
  TK_KW_FALSE,
  TK_KW_FN,
  TK_KW_IF,
  TK_KW_IMPORT,
  TK_KW_IS,
  TK_KW_LOOP,
  TK_KW_MAIN,
  TK_KW_MUT,
  TK_KW_NIL,
  TK_KW_OBJ,
  TK_KW_RETURN,
  TK_KW_THROW,
  TK_KW_TRUE,
  TK_KW_TRY,
  TK_KW_UNION,

  TK_LIT_CHAR,
  TK_LIT_FLOAT,
  TK_LIT_INT_BIN,
  TK_LIT_INT_DEC,
  TK_LIT_INT_HEX,
  TK_LIT_INT_OCT,
  TK_LIT_STR,

  TK_OP_AMP,
  TK_OP_AMP_EQ,
  TK_OP_AMP_AMP,
  TK_OP_AMP_AMP_EQ,
  TK_OP_AT,
  TK_OP_BACKTICK,
  TK_OP_BACKSLASH,
  TK_OP_CARET,
  TK_OP_CARET_EQ,
  TK_OP_COLON,
  TK_OP_COLON_EQ,
  TK_OP_COMMA,
  TK_OP_DOT,
  TK_OP_DOLLAR,
  TK_OP_ELLIPSIS,
  TK_OP_EQ,
  TK_OP_EQ_EQ,
  TK_OP_EXCL,
  TK_OP_EXCL_EQ,
  TK_OP_GT,
  TK_OP_GT_EQ,
  TK_OP_HASH,
  TK_OP_LBRACE,
  TK_OP_LBRACK,
  TK_OP_LPAR,
  TK_OP_LSHIFT,
  TK_OP_LSHIFT_EQ,
  TK_OP_LT,
  TK_OP_LT_EQ,
  TK_OP_MINUS,
  TK_OP_MINUS_EQ,
  TK_OP_MINUS_MINUS,
  TK_OP_PIPE,
  TK_OP_PIPE_EQ,
  TK_OP_PIPE_PIPE,
  TK_OP_PIPE_PIPE_EQ,
  TK_OP_PERCENT,
  TK_OP_PERCENT_EQ,
  TK_OP_PLUS,
  TK_OP_PLUS_EQ,
  TK_OP_PLUS_PLUS,
  TK_OP_QN,
  TK_OP_RBRACE,
  TK_OP_RBRACK,
  TK_OP_RPAR,
  TK_OP_RSHIFT,
  TK_OP_RSHIFT_EQ,
  TK_OP_SEMI,
  TK_OP_SLASH,
  TK_OP_SLASH_EQ,
  TK_OP_STAR,
  TK_OP_STAR_EQ,
  TK_OP_TILDE
}

obj Tokenizer {
  reader: ref Reader
  state: TokenizerState
  data: Token[]
  errors: str[]
}

obj TokenizerState {
  idx: int
  pos: int
  ch: char
  handled: bool
}

fn Arguments_init () Arguments {
  if process.args.len == 1 {
    RaiseError("REPL is not supported")
  }

  mut fileName: str?

  loop i := 1; i < process.args.len; i++ {
    arg := process.args[i]

    if arg[0] == '-' {
      RaiseError("Bad option " + arg)
    } elif fileName == nil {
      fileName = arg
    } else {
      RaiseError("Processing multiple files is not supported")
    }
  }

  if fileName == nil {
    RaiseError("File is not specified")
  }

  return Arguments{fileName: fileName}
}

fn E0000 (unexpected: char, expected: char?) str {
  return "E0000: Unexpected token `" + unexpected.str() + "`" +
    (expected == nil ? "" : ", expected.str() `" + expected + "`")
}

fn E0001 () str {
  return "E0001 - Unterminated comment"
}

fn E0002 () str {
  return "E0002 - Unterminated character literal"
}

fn E0003 () str {
  return "E0003 - Unterminated string literal"
}

fn E0004 () str {
  return "E0004 - Empty character literal"
}

fn E0005 () str {
  return "E0005 - Illegal character escape sequence"
}

fn E0006 () str {
  return "E0006 - Illegal string escape sequence"
}

fn E0007 () str {
  return "E0007 - Too many characters in character literal"
}

fn E0008 () str {
  return "E0008 - Leading zero integer literals are not allowed"
}

fn E0009 (type: str) str {
  return "E0009 - Invalid " + type + " integer literal"
}

fn E0010 () str {
  return "E0010 - Invalid float literal"
}

fn E0011 () str {
  return "E0011 - Invalid float literal exponent"
}

fn E0012 (type: str) str {
  return "E0012 - " + type.toUpperFirst() + " float literals are not allowed"
}

fn Location_locate (content: str, pos: int) Location {
  if pos == 0 {
    return Location{line: 1, col: 1}
  }

  lines := content.lines()
  mut curPos := 0

  loop i := 0; i < lines.len; i++ {
    line := lines[i]

    if curPos + line.len > pos {
      return Location{line: i + 1, col: pos - curPos + 1}
    }

    curPos += line.len
  }

  return Location{line: lines.len, col: lines[lines.len - 1].len}
}

fn Reader_init (p: str) Reader {
  path := realpathSync(p)

  if !isFileSync(path) {
    RaiseError("Path \"" + path + "\" is not a file")
  }

  content := readFileSync(path, "utf8")
  return Reader{path: path, content: content}
}

fn Reader_eof (this: ref Reader) bool {
  return this.pos >= this.content.len
}

fn Reader_lookahead (mut this: ref Reader, check: char) bool {
  if this.pos + 1 >= this.content.len {
    return false
  }

  ch := this.content[this.pos + 1]

  if (
    OS == "Windows" &&
    ch == '\r' &&
    this.pos + 2 < this.content.len &&
    this.content[this.pos + 2] == '\n'
  ) {
    if check == '\n' {
      this.pos += 2
      return true
    }
  } elif check == ch {
    this.pos++
    return true
  }

  return false
}

fn Reader_next (mut this: ref Reader) char {
  if Reader_eof(this) {
    RaiseError("Tried to read on eof")
  }

  ch1 := this.content[this.pos]
  this.pos++

  if OS == "Windows" && ch1 == '\r' && !Reader_eof(this) {
    ch2 := this.content[this.pos]

    if ch2 == '\n' {
      this.pos++
      return ch2
    }
  }

  return ch1
}

fn Reader_seek (mut this: ref Reader, pos: int) {
  this.pos = pos
}

fn Reader_slice (this: ref Reader, start: int?, end: int?) str {
  return this.content.slice(start, end)
}

fn Reader_walk (mut this: ref Reader, match: fn (char) bool) {
  loop !Reader_eof(this) {
    pos := this.pos
    ch := Reader_next(this)

    if !match(ch) {
      Reader_seek(this, pos)
      break
    }
  }
}

fn Token_isCharEsc (ch: char) bool {
  return
    ch == '0' ||
    ch == 'b' ||
    ch == 'f' ||
    ch == 'n' ||
    ch == 'r' ||
    ch == 't' ||
    ch == 'v' ||
    ch == '"' ||
    ch == '\'' ||
    ch == '\\'
}

fn Token_isId (ch: char) bool {
  return ch.isAlphaNum() || ch == '_'
}

fn Token_isIdStart (ch: char) bool {
  return ch.isAlpha() || ch == '_'
}

fn Token_isIntBin (ch: char) bool {
  return ch == '0' || ch == '1'
}

fn Token_isIntDec (ch: char) bool {
  return ch.isDigit()
}

fn Token_isIntHex (ch: char) bool {
  return
    ch.isDigit() ||
    ch == 'A' || ch == 'a' ||
    ch == 'B' || ch == 'b' ||
    ch == 'C' || ch == 'c' ||
    ch == 'D' || ch == 'd' ||
    ch == 'E' || ch == 'e' ||
    ch == 'F' || ch == 'f'
}

fn Token_isIntOct (ch: char) bool {
  return
    ch == '0' ||
    ch == '1' ||
    ch == '2' ||
    ch == '3' ||
    ch == '4' ||
    ch == '5' ||
    ch == '6' ||
    ch == '7'
}

fn Token_isNotNewLine (ch: char) bool {
  return ch != '\n'
}

fn Token_isStrEsc (ch: char) bool {
  return Token_isCharEsc(ch)
}

fn Token_str (this: Token) str {
  val := _Token_valEscaped(this)

  return
    this.type.str().slice(3) +
    "(" + this.start.str() + "-" + this.end.str() + ")" +
    (val == "" ? "" : ": " + val)
}

fn Token_xml (this: Token) str {
  return
    "<Token" +
    " type=\"" + this.type.str().slice(3) + "\"" +
    " val=\"" + _Token_valEscaped(this, true) + "\"" +
    " start=\"" + this.start.str() + "\"" +
    " end=\"" + this.end.str() + "\"" +
    " />"
}

fn _Token_valEscaped (this: Token, xml := false) str {
  mut result := ""

  loop i := 0; i < this.val.len; i++ {
    ch := this.val[i]

    if ch == '\f' {
      result += "\\f"
    } elif ch == '\n' {
      result += "\\n"
    } elif ch == '\r' {
      result += "\\r"
    } elif ch == '\t' {
      result += "\\t"
    } elif ch == '\v' {
      result += "\\v"
    } elif ch == '"' && xml {
      result += "\\\""
    } else {
      result += ch
    }
  }

  return result
}

fn Tokenizer_init (mut reader: ref Reader) Tokenizer {
  return Tokenizer{
    reader: reader,
    state: TokenizerState{
      idx: 0,
      pos: 0,
      ch: '\0',
      handled: false
    },
    data: [],
    errors: []
  }
}

fn Tokenizer_next (
  mut this: ref Tokenizer,
  withIgnored := false
) Token {
  if this.data.len < this.state.idx {
    return this.data[this.state.idx++]
  } elif this.data[this.data.len - 1].type == TK_EOF {
    RaiseError("Tried to tokenize on eof")
  }

  loop {
    tok := _Tokenizer_getToken(this)

    this.data.push(tok)
    this.state.idx++

    if withIgnored || (
      tok.type != TK_WHITESPACE &&
      tok.type != TK_COMMENT_BLOCK &&
      tok.type != TK_COMMENT_LINE
    ) {
      return tok
    }
  }
}

fn _Tokenizer_getToken (mut this: ref Tokenizer) Token {
  if Reader_eof(this.reader) {
    return Token{TK_EOF, "", this.state.pos, this.state.pos}
  }

  this.state.ch = Reader_next(this.reader)
  this.state.handled = false
  mut tok: Token?

  if (tok = _Tokenizer_maybeWhitespace(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeCommentBlock(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeCommentLine(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeOp(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeKeyword(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeNumber(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeString(this)) != nil {
    return tok
  } elif (tok = _Tokenizer_maybeChar(this)) != nil {
    return tok
  } else {
    _Tokenizer_raise(this, E0000(ch), this.state.pos)
    return _Tokenizer_wrapToken(this, TK_UNKNOWN)
  }
}

fn _Tokenizer_maybeChar (mut this: ref Tokenizer) Token? {
  if this.state.ch != '\'' {
    return nil
  }

  if Reader_eof(this.reader) {
    _Tokenizer_raise(this, E0002(), this.state.pos)
    return _Tokenizer_wrapToken(this, TK_LIT_CHAR)
  }

  ch1 := Reader_next(this.reader)

  if ch1 == '\n' {
    _Tokenizer_raise(this, E0002(), this.state.pos)
    return _Tokenizer_wrapToken(this, TK_LIT_CHAR)
  } elif ch1 == '\'' {
    _Tokenizer_raise(this, E0004(), this.state.pos)
    return _Tokenizer_wrapToken(this, TK_LIT_CHAR)
  } elif ch1 == '\\' {
    if Reader_eof(this.reader) {
      _Tokenizer_raise(this, E0002(), this.state.pos)
      return _Tokenizer_wrapToken(this, TK_LIT_CHAR)
    }

    ch2 := Reader_next(this.reader)

    if !Token_isCharEsc(ch2) {
      _Tokenizer_raise(this, E0005(), this.state.pos)
    }
  }

  if !Reader_lookahead(this.reader, '\'') {
    loop {
      if Reader_eof(this.reader) {
        _Tokenizer_raise(this, E0002(), this.state.pos)
        break
      }

      pos3 := this.reader.pos
      ch3 := Reader_next(this.reader)

      if ch3 == '\'' {
        _Tokenizer_raise(this, E0007(), this.state.pos)
        break
      } elif ch3 == '\n' {
        Reader_seek(this.reader, pos3)
        _Tokenizer_raise(this, E0002(), this.state.pos)

        break
      }
    }
  }

  return _Tokenizer_wrapToken(this, TK_LIT_CHAR)
}

fn _Tokenizer_maybeCommentBlock (mut this: ref Tokenizer) Token? {
  if this.state.ch != '/' || !Reader_lookahead(this.reader, '*') {
    return nil
  }

  loop {
    if Reader_eof(this.reader) {
      _Tokenizer_raise(this, E0001(), this.state.pos)
      break
    }

    ch := Reader_next(this.reader)

    if ch == '*' && Reader_lookahead(this.reader, '/') {
      break
    }
  }

  return _Tokenizer_wrapToken(this, TK_COMMENT_BLOCK)
}

fn _Tokenizer_maybeCommentLine (mut this: ref Tokenizer) Token? {
  if this.state.ch != '/' || !Reader_lookahead(this.reader, '/') {
    return nil
  }

  Reader_walk(this.reader, Token_isNotNewLine)
  return _Tokenizer_wrapToken(this, TK_COMMENT_LINE)
}

fn _Tokenizer_maybeKeyword (mut this: ref Tokenizer) Token? {
  if !Token_isIdStart(this.state.ch) {
    return nil
  }

  Reader_walk(this.reader, Token_isId)
  val := Reader_slice(this.reader, this.state.pos, this.reader.pos)

  if val == "break" {
    return _Tokenizer_wrapToken(this, TK_KW_BREAK)
  } elif val == "catch" {
    return _Tokenizer_wrapToken(this, TK_KW_CATCH)
  } elif val == "continue" {
    return _Tokenizer_wrapToken(this, TK_KW_CONTINUE)
  } elif val == "elif" {
    return _Tokenizer_wrapToken(this, TK_KW_ELIF)
  } elif val == "else" {
    return _Tokenizer_wrapToken(this, TK_KW_ELSE)
  } elif val == "enum" {
    return _Tokenizer_wrapToken(this, TK_KW_ENUM)
  } elif val == "export" {
    return _Tokenizer_wrapToken(this, TK_KW_EXPORT)
  } elif val == "false" {
    return _Tokenizer_wrapToken(this, TK_KW_FALSE)
  } elif val == "fn" {
    return _Tokenizer_wrapToken(this, TK_KW_FN)
  } elif val == "if" {
    return _Tokenizer_wrapToken(this, TK_KW_IF)
  } elif val == "import" {
    return _Tokenizer_wrapToken(this, TK_KW_IMPORT)
  } elif val == "is" {
    return _Tokenizer_wrapToken(this, TK_KW_IS)
  } elif val == "loop" {
    return _Tokenizer_wrapToken(this, TK_KW_LOOP)
  } elif val == "main" {
    return _Tokenizer_wrapToken(this, TK_KW_MAIN)
  } elif val == "mut" {
    return _Tokenizer_wrapToken(this, TK_KW_MUT)
  } elif val == "nil" {
    return _Tokenizer_wrapToken(this, TK_KW_NIL)
  } elif val == "obj" {
    return _Tokenizer_wrapToken(this, TK_KW_OBJ)
  } elif val == "return" {
    return _Tokenizer_wrapToken(this, TK_KW_RETURN)
  } elif val == "throw" {
    return _Tokenizer_wrapToken(this, TK_KW_THROW)
  } elif val == "true" {
    return _Tokenizer_wrapToken(this, TK_KW_TRUE)
  } elif val == "try" {
    return _Tokenizer_wrapToken(this, TK_KW_TRY)
  } elif val == "union" {
    return _Tokenizer_wrapToken(this, TK_KW_UNION)
  }

  return _Tokenizer_wrapToken(this, TK_ID)
}

fn _Tokenizer_maybeNumber (mut this: ref Tokenizer) Token? {
  if !this.state.ch.isDigit() {
    return nil
  }

  if this.state.ch == '0' {
    if Reader_eof(this.reader) {
      return _Tokenizer_wrapToken(this, TK_LIT_INT_DEC)
    }

    pos := this.reader.pos
    ch := Reader_next(this.reader)

    if Token_isIntDec(ch) {
      Reader_walk(this.reader, Token_isId)
      _Tokenizer_raise(this, E0008(), this.state.pos)

      return _Tokenizer_wrapToken(this, TK_LIT_INT_DEC)
    } elif ch == 'B' || ch == 'b' {
      return _Tokenizer_wrapInt(
        this,
        TK_LIT_INT_BIN,
        E0009("binary"),
        Token_isIntBin
      )
    } elif ch == 'O' || ch == 'o' {
      return _Tokenizer_wrapInt(
        this,
        TK_LIT_INT_OCT,
        E009("octal"),
        Token_isIntOct
      )
    } elif ch == 'X' || ch == 'x' {
      return _Tokenizer_wrapInt(
        this,
        TK_LIT_INT_HEX,
        E0009("hexadecimal"),
        Token_isIntHex
      )
    }

    Reader_seek(this.reader, pos)
  } else {
    Reader_walk(this.reader, Token_isIntDec)
  }

  return _Tokenizer_wrapInt(
    this,
    TK_LIT_INT_DEC,
    E009("decimal"),
    Token_isIntDec
  )
}

fn _Tokenizer_maybeOp (mut this: ref Tokenizer) Token? {
  if this.state.ch == '&' {
    if Reader_lookahead(this.reader, '&') {
      if Reader_lookahead(this.reader, '=') {
        return _Tokenizer_wrapToken(this, TK_OP_AMP_AMP_EQ)
      } else {
        return _Tokenizer_wrapToken(this, TK_OP_AMP_AMP)
      }
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_AMP_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_AMP)
    }
  } elif this.state.ch == '@' {
    return _Tokenizer_wrapToken(this, TK_OP_AT)
  } elif this.state.ch == '`' {
    return _Tokenizer_wrapToken(this, TK_OP_BACKTICK)
  } elif this.state.ch == '\\' {
    return _Tokenizer_wrapToken(this, TK_OP_BACKSLASH)
  } elif this.state.ch == '^' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_CARET_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_CARET)
    }
  } elif this.state.ch == ':' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_COLON_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_COLON)
    }
  } elif this.state.ch == ',' {
    return _Tokenizer_wrapToken(this, TK_OP_COMMA)
  } elif this.state.ch == '.' {
    pos := this.reader.pos

    if (
      Reader_lookahead(this.reader, '.') &&
      Reader_lookahead(this.reader, '.')
    ) {
      return _Tokenizer_wrapToken(this, TK_OP_ELLIPSIS)
    }

    Reader_seek(this.reader, pos)
    return _Tokenizer_wrapToken(this, TK_OP_DOT)
  } elif this.state.ch == '$' {
    return _Tokenizer_wrapToken(this, TK_OP_DOLLAR)
  } elif this.state.ch == '=' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_EQ_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_EQ)
    }
  } elif this.state.ch == '!' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_EXCL_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_EXCL)
    }
  } elif this.state.ch == '>' {
    if Reader_lookahead(this.reader, '>') {
      if Reader_lookahead(this.reader, '=') {
        return _Tokenizer_wrapToken(this, TK_OP_RSHIFT_EQ)
      } else {
        return _Tokenizer_wrapToken(this, TK_OP_RSHIFT)
      }
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_GT_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_GT)
    }
  } elif this.state.ch == '#' {
    return _Tokenizer_wrapToken(this, TK_OP_HASH)
  } elif this.state.ch == '{' {
    return _Tokenizer_wrapToken(this, TK_OP_LBRACE)
  } elif this.state.ch == '[' {
    return _Tokenizer_wrapToken(this, TK_OP_LBRACK)
  } elif this.state.ch == '(' {
    return _Tokenizer_wrapToken(this, TK_OP_LPAR)
  } elif this.state.ch == '<' {
    if Reader_lookahead(this.reader, '<') {
      if Reader_lookahead(this.reader, '=') {
        return _Tokenizer_wrapToken(this, TK_OP_LSHIFT_EQ)
      } else {
        return _Tokenizer_wrapToken(this, TK_OP_LSHIFT)
      }
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_LT_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_LT)
    }
  } elif this.state.ch == '-' {
    if Reader_lookahead(this.reader, '-') {
      return _Tokenizer_wrapToken(this, TK_OP_MINUS_MINUS)
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_MINUS_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_MINUS)
    }
  } elif this.state.ch == '|' {
    if Reader_lookahead(this.reader, '|') {
      if Reader_lookahead(this.reader, '=') {
        return _Tokenizer_wrapToken(this, TK_OP_PIPE_PIPE_EQ)
      } else {
        return _Tokenizer_wrapToken(this, TK_OP_PIPE_PIPE)
      }
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_PIPE_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_PIPE)
    }
  } elif this.state.ch == '%' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_PERCENT_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_PERCENT)
    }
  } elif this.state.ch == '+' {
    if Reader_lookahead(this.reader, '+') {
      return _Tokenizer_wrapToken(this, TK_OP_PLUS_PLUS)
    } elif Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_PLUS_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_PLUS)
    }
  } elif this.state.ch == '?' {
    return _Tokenizer_wrapToken(this, TK_OP_QN)
  } elif this.state.ch == '}' {
    return _Tokenizer_wrapToken(this, TK_OP_RBRACE)
  } elif this.state.ch == ']' {
    return _Tokenizer_wrapToken(this, TK_OP_RBRACK)
  } elif this.state.ch == ')' {
    return _Tokenizer_wrapToken(this, TK_OP_RPAR)
  } elif this.state.ch == ';' {
    return _Tokenizer_wrapToken(this, TK_OP_SEMI)
  } elif this.state.ch == '/' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_SLASH_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_SLASH)
    }
  } elif this.state.ch == '*' {
    if Reader_lookahead(this.reader, '=') {
      return _Tokenizer_wrapToken(this, TK_OP_STAR_EQ)
    } else {
      return _Tokenizer_wrapToken(this, TK_OP_STAR)
    }
  } elif this.state.ch == '~' {
    return _Tokenizer_wrapToken(this, TK_OP_TILDE)
  }

  return nil
}

fn _Tokenizer_maybeString (mut this: ref Tokenizer) Token? {
  if this.state.ch != '"' {
    return nil
  }

  loop {
    if Reader_eof(this.reader) {
      _Tokenizer_raise(this, E0003(), this.state.pos)
      break
    }

    pos1 := this.reader.pos
    ch1 := Reader_next(this.reader)

    if ch1 == '"' {
      break
    } elif ch1 == '\\' {
      ch2 := Reader_next(this.reader)

      if !Token_isStrEsc(ch2) {
        _Tokenizer_raise(this, E0006(), pos1)
      }
    }
  }

  return _Tokenizer_wrapToken(this, TK_LIT_STR)
}

fn _Tokenizer_maybeWhitespace (mut this: ref Tokenizer) Token? {
  if !this.state.ch.isSpace() {
    return nil
  }

  Reader_walk(this.reader, Token_isWhitespace)
  return _Tokenizer_wrapToken(this, TK_WHITESPACE)
}

fn _Tokenizer_raise (mut this: ref Tokenizer, message: str, start: int) {
  if this.state.handled {
    return
  }

  startLoc := Location_locate(this.reader.content, start)
  endLoc := Location_locate(this.reader.content, this.reader.pos)
  lines := this.reader.content.lines()

  mut errorLines: str[]
  mut errorLinesNums: int[]

  if startLoc.line != 1 {
    errorLines.push(lines[startLoc.line - 2])
    errorLinesNums.push(startLoc.line - 2)
  }

  errorLines.push(lines[startLoc.line - 1])
  errorLinesNums.push(startLoc.line - 1)

  if startLoc.line == endLoc.line && startLoc.col == endLoc.col {
    errorLines.push(' '.repeat(startLoc.col - 1) + '^')
  } elif startLoc.line == endLoc.line {
    errorLines.push(
      ' '.repeat(startLoc.col - 1) +
      '~'.repeat(endLoc.col - startLoc.col)
    )
  } else {
    errorLine := lines[startLoc.line - 1]

    errorLines.push(
      ' '.repeat(startLoc.col - 1) +
      '~'.repeat(errorLine.len - startLoc.col)
    )
  }

  errorLinesNums.push(startLoc.line - 1)

  if startLoc.line != lines.len {
    errorLines.push(lines[startLoc.line])
    errorLinesNums.push(startLoc.line)
  }

  gutterLen := errorLinesNums[errorLinesNums.len - 1].str().len + 1

  mut error := this.reader.path + ":" + startLoc.line.str() + ":" +
    startLoc.col.str() + ": SyntaxError: " + message + EOL

  mut prevLineNum := 0

  loop i := 0; i < errorLinesNums.len; i++ {
    errorLineNum := errorLinesNums[i]
    sameLine := prevLineNum == errorLineNum

    if sameLine {
      error += ' '.repeat(gutterLen)
    } else {
      errorLineNumLen := errorLineNum.str().len
      error += errorLineNum.str() + ' '.repeat(gutterLen - errorLineNumLen)
    }

    error += "| " + errorLines[i]
    prevLineNum = errorLineNum
  }

  this.errors.push(error)
  this.state.handled = true
}

fn _Tokenizer_wrapFloat (
  mut this: ref Tokenizer,
  type: TokenType,
  errorMessage: str
) Token {
  if Reader_eof(this.reader) {
    return _Tokenizer_wrapToken(this, type)
  }

  pos1 := this.reader.pos
  ch1 := Reader_next(this.reader)

  if Token_isId(ch1) && ch1 != 'E' && ch1 != 'e' {
    Reader_walk(this.reader, Token_isId)
    _Tokenizer_raise(this, errorMessage, this.state.pos)

    return _Tokenizer_wrapToken(this, type)
  } elif ch1 != '.' && ch1 != 'E' && ch1 != 'e' {
    Reader_seek(this.reader, pos1)
    return _Tokenizer_wrapToken(this, type)
  }

  mut expStartPos := pos1

  if ch1 == '.' {
    if Reader_eof(this.reader) {
      _Tokenizer_raise(this, E0010(), this.state.pos)
      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    }

    pos2 := this.reader.pos
    ch2 := Reader_next(this.reader)

    if ch2 == '.' {
      Reader_seek(this.reader, pos1)
      return _Tokenizer_wrapToken(this, type)
    } elif !Token_isIntDec(ch2) {
      Reader_walk(this.reader, Token_isId)
      _Tokenizer_raise(this, E0010(), this.state.pos)

      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    }

    Reader_walk(this.reader, Token_isIntDec)

    if Reader_eof(this.reader) {
      return _Tokenizer_wrapTokenFloat(this, type)
    }

    pos3 := this.reader.pos
    ch3 := Reader_next(this.reader)

    if Token_isId(ch3) && ch3 != 'E' && ch3 != 'e' {
      Reader_walk(this.reader, Token_isId)
      _Tokenizer_raise(this, E0010(), this.state.pos)

      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    } elif ch3 != 'E' && ch3 != 'e' {
      Reader_seek(this.reader, pos2)
      return _Tokenizer_wrapTokenFloat(this, type)
    }

    expStartPos = pos3
  }

  if Reader_eof(this.reader) {
    _Tokenizer_raise(this, E0011(), expStartPos)
    return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
  }

  ch4 := Reader_next(this.reader)

  if !Token_isIntDec(ch4) && ch4 != '+' && ch4 != '-' {
    Reader_walk(this.reader, Token_isId)
    _Tokenizer_raise(this, E0011(), expStartPos)

    return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
  }

  if ch4 == '+' || ch4 == '-' {
    if Reader_eof(this.reader) {
      _Tokenizer_raise(this, E0011(), expStartPos)
      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    }

    ch5 := Reader_next(this.reader)

    if !Token_isIntDec(ch5) {
      Reader_walk(this.reader, Token_isId)
      _Tokenizer_raise(this, E0011(), expStartPos)

      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    }
  }

  Reader_walk(this.reader, Token_isIntDec)
  return _Tokenizer_wrapTokenFloat(this, type)
}

fn _Tokenizer_wrapInt (
  mut this: ref Tokenizer,
  type: TokenType,
  errorMessage: str,
  check: fn (char) bool
) Token {
  if Reader_eof(this.reader) {
    _Tokenizer_raise(this, errorMessage, this.state.pos)
    return _Tokenizer_wrapToken(this, type)
  }

  ch := Reader_next(this.reader)

  if !check(ch) {
    Reader_walk(this.reader, Token_isId)
    _Tokenizer_raise(this, errorMessage, this.state.pos)

    return _Tokenizer_wrapToken(this, type)
  }

  Reader_walk(this.reader, check)
  return _Tokenizer_wrapFloat(this, type, errorMessage)
}

fn _Tokenizer_wrapToken (mut this: ref Tokenizer, type: TokenType) Token {
  start := this.state.pos
  this.state.pos = this.reader.pos
  this.state.handled = true

  return Token{
    type: type,
    val: Reader_slice(this.reader, start, this.state.pos),
    start: start,
    end: this.state.pos
  }
}

fn _Tokenizer_wrapTokenFloat (mut this: ref Tokenizer, type: TokenType) Token {
  if !Reader_eof(this.reader) {
    ch := Reader_next(this.reader)

    if Token_isId(ch) {
      Reader_walk(this.reader, Token_isId)
      _Tokenizer_raise(this, E0010(), this.state.pos)

      return _Tokenizer_wrapToken(this, TK_LIT_FLOAT)
    }

    Reader_seek(this.reader, pos)
  }

  if type == TK_LIT_INT_BIN {
    _Tokenizer_raise(this, E0014("binary"), this.state.pos)
  } elif type == TK_LIT_INT_HEX {
    _Tokenizer_raise(this, E0014("hexadecimal"), this.state.pos)
  } elif type == TK_LIT_INT_OCT {
    _Tokenizer_raise(this, E0014("octal"), this.state.pos)
  }

  return _Tokenizer_wrapToken(TK_LIT_FLOAT)
}

main {
  arguments := Arguments_init()
  mut reader := Reader_init(arguments.fileName)
  mut tokenizer := Tokenizer_init(ref reader)
  mut result: str[]

  loop (
    tok := Tokenizer_next(ref tokenizer);
    tok.type != TK_EOF;
    tok = Tokenizer_next(ref tokenizer)
  ) {
    result.push(Token_str(tok))
  }

  if tokenizer.errors.len != 0 {
    RaiseError(tokenizer.errors.join(EOL + EOL))
  } else {
    print(result.join(EOL))
  }
}
