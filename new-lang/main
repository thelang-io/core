/*!
 * Copyright (c) Aaron Delasy
 *
 * Unauthorized copying of this file, via any medium is strictly prohibited
 * Proprietary and confidential
 */

import Error, NewError from "error"
import * as os from "os"

import Arguments_init from "./arguments"
import Reader, Reader_init from "./reader"
import TokenType, Token_str from "./token"
import Tokenizer_init, Tokenizer_next from "./tokenizer"

main {
  try {
    arguments := Arguments_init()
    mut reader := Reader_init(arguments.fileName)
    mut tokenizer := Tokenizer_init(ref reader)
    mut result: str[]

    loop (
      tok := Tokenizer_next(ref tokenizer);
      tok.type != TK_EOF;
      tok = Tokenizer_next(ref tokenizer)
    ) {
      result.push(Token_str(tok))
    }

    if tokenizer.errors.len != 0 {
      throw NewError(
        tokenizer.errors.join(os.lineSeparator + os.lineSeparator)
      )
    } else {
      print(result.join(os.lineSeparator))
    }
  } catch err: Error {
    print("Error: " + err.message, to: process.stderr)
    exit(1)
  }
}
